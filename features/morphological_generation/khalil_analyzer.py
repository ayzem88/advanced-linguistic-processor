#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ù…Ø­Ù„Ù„ Ø§Ù„Ø®Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù…Ù†Ù‡Ø¬ Ø§Ù„Ø£ØµÙ„ÙŠ
Khalil Morphological Analyzer - Final Version Matching Original Methodology
"""

import sys
import os
import xml.etree.ElementTree as ET
import re
import logging
from typing import List, Dict, Tuple, Optional

class KhalilAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ø®Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"""
    
    def __init__(self):
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        
        # Ø¥Ù†Ø´Ø§Ø¡ handler Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
        
        self.db_path = os.path.join(os.path.dirname(__file__), 'db')
        
        # ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
        try:
            self.prefixes = self._load_prefixes()
            self.suffixes = self._load_suffixes()
            self.patterns = self._load_patterns()
            self.roots = self._load_roots()
            self.toolwords = self._load_toolwords()

            # Ø®Ø±Ø§Ø¦Ø· Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ ÙØ¦Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©/Ø§Ù„Ù„Ø§Ø­Ù‚Ø© Ø¨Ø³Ø±Ø¹Ø©
            self._pref_class = {p.get('unvoweled'): (p.get('class') or '') for p in self.prefixes if p.get('unvoweled') is not None}
            self._suf_class = {s.get('unvoweled'): (s.get('class') or '') for s in self.suffixes if s.get('unvoweled') is not None}
            
            self.logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­:")
            self.logger.info(f"   ğŸ“ Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª: {len(self.prefixes)}")
            self.logger.info(f"   ğŸ“ Ø§Ù„Ù„ÙˆØ§Ø­Ù‚: {len(self.suffixes)}")
            self.logger.info(f"   ğŸ“ Ø§Ù„Ø£Ù†Ù…Ø§Ø·: {len(self.patterns)}")
            self.logger.info(f"   ğŸ“ Ø§Ù„Ø¬Ø°ÙˆØ±: {len(self.roots)}")
            self.logger.info(f"   ğŸ“ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©: {len(self.toolwords)}")
            
        except Exception as e:
            self.logger.error(f"ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
            raise RuntimeError(f"Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµØ±ÙÙŠØ©: {e}")
    
    def _load_xml_file(self, file_path: str) -> ET.ElementTree:
        """
        ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù XML Ù…Ø¹ Ø¯Ø¹Ù… ØªØ±Ù…ÙŠØ²Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
        
        Args:
            file_path: Ù…Ø³Ø§Ø± Ù…Ù„Ù XML
            
        Returns:
            ElementTree: Ø´Ø¬Ø±Ø© XML Ø§Ù„Ù…Ø­Ù…Ù„Ø©
            
        Raises:
            ValueError: Ø¥Ø°Ø§ ÙØ´Ù„ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ±Ù…ÙŠØ²Ø§Øª
        """
        encodings = ['utf-8', 'utf-8-sig', 'windows-1256', 'cp1256', 'iso-8859-6']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    tree = ET.parse(f)
                    self.logger.debug(f"ØªÙ… ØªØ­Ù…ÙŠÙ„ {file_path} Ø¨ØªØ±Ù…ÙŠØ² {encoding}")
                    return tree
            except UnicodeDecodeError:
                self.logger.debug(f"ÙØ´Ù„ ØªØ±Ù…ÙŠØ² {encoding} Ù„Ù„Ù…Ù„Ù {file_path}")
                continue
            except ET.ParseError as e:
                self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ XML Ù„Ù„Ù…Ù„Ù {file_path}: {e}")
                raise
            except FileNotFoundError:
                self.logger.error(f"Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}")
                raise
        
        raise ValueError(f"Ù„Ø§ ÙŠÙ…ÙƒÙ† Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù {file_path} Ø¨Ø£ÙŠ Ù…Ù† Ø§Ù„ØªØ±Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©")
    
    def _load_prefixes(self) -> List[Dict]:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø¯Ø¹Ù… ØªØ±Ù…ÙŠØ²Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©"""
        prefixes = []
        try:
            tree = self._load_xml_file(os.path.join(self.db_path, 'prefixes.xml'))
            root = tree.getroot()
            
            for prefix in root.findall('prefixe'):
                unvoweled = prefix.get('unvoweledform', '').strip()
                voweled = prefix.get('voweledform', '').strip()
                classe = prefix.get('classe', '').strip()
                
                if unvoweled or classe:  # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ© Ø£ÙŠØ¶Ø§Ù‹
                    prefixes.append({
                        'unvoweled': unvoweled,
                        'voweled': voweled,
                        'class': classe
                    })
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª: {e}")
            raise
        
        return prefixes
    
    def _load_suffixes(self) -> List[Dict]:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù„ÙˆØ§Ø­Ù‚ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø¯Ø¹Ù… ØªØ±Ù…ÙŠØ²Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©"""
        suffixes = []
        try:
            tree = self._load_xml_file(os.path.join(self.db_path, 'suffixes.xml'))
            root = tree.getroot()
            
            for suffix in root.findall('suffixe'):
                unvoweled = suffix.get('unvoweledform', '').strip()
                voweled = suffix.get('voweledform', '').strip()
                classe = suffix.get('classe', '').strip()
                
                if unvoweled or classe:  # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ù„ÙˆØ§Ø­Ù‚ Ø§Ù„ÙØ§Ø±ØºØ© Ø£ÙŠØ¶Ø§Ù‹
                    suffixes.append({
                        'unvoweled': unvoweled,
                        'voweled': voweled,
                        'class': classe
                    })
        except Exception as e:
            self.logger.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù„ÙˆØ§Ø­Ù‚: {e}")
            raise
        # Ø¶Ù…Ø§Ù† ØªÙˆÙØ± Ø¨Ø¹Ø¶ Ø§Ù„Ù„ÙˆØ§Ø­Ù‚ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø¥Ù† ÙƒØ§Ù†Øª ØºØ§Ø¦Ø¨Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        ensure_suffixes = [
            {'unvoweled': 'ÙˆÙ†', 'voweled': 'ÙˆÙ†Ù', 'class': 'C2'},
            {'unvoweled': 'ÙŠÙ†', 'voweled': 'ÙŠÙ†Ù', 'class': 'C2'},
            {'unvoweled': 'Ø§Øª', 'voweled': 'Ø§ØªÙŒ', 'class': 'C2'},
        ]
        existing = set(s.get('unvoweled') for s in suffixes)
        for s in ensure_suffixes:
            if s['unvoweled'] not in existing:
                suffixes.append(s)
                existing.add(s['unvoweled'])

        return suffixes
    
    def _load_patterns(self) -> List[Dict]:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ù…Ù† Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© (Unvoweled/Voweled)"""
        patterns: List[Dict] = []
        try:
            base_dirs = [
                os.path.join(self.db_path, 'nouns', 'patterns', 'Unvoweled'),
                os.path.join(self.db_path, 'nouns', 'patterns', 'Voweled'),
                os.path.join(self.db_path, 'verbs', 'patterns', 'Unvoweled'),
                os.path.join(self.db_path, 'verbs', 'patterns', 'Voweled'),
            ]
            for d in base_dirs:
                if not os.path.isdir(d):
                    continue
                for fname in os.listdir(d):
                    if not fname.lower().endswith('.xml'):
                        continue
                    fpath = os.path.join(d, fname)
                    try:
                        tree = ET.parse(fpath)
                        root = tree.getroot()
                        for pattern in root.findall('pattern'):
                            patterns.append({
                                'id': pattern.get('id', ''),
                                'diac': pattern.get('diac', ''),
                                'type': pattern.get('type', ''),
                                'aug': pattern.get('aug', ''),
                                'cas': pattern.get('cas', ''),
                                'ncg': pattern.get('ncg', ''),
                                'trans': pattern.get('trans', '')
                            })
                    except Exception:
                        continue
        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø·: {e}")
        return patterns
    
    def _load_roots(self) -> List[Dict]:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¬Ø°ÙˆØ± Ù…Ù† Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª (nouns/roots/*.xml, verbs/roots/*.xml)"""
        roots: List[Dict] = []
        try:
            base_dirs = [
                os.path.join(self.db_path, 'nouns', 'roots'),
                os.path.join(self.db_path, 'verbs', 'roots'),
            ]
            for d in base_dirs:
                if not os.path.isdir(d):
                    continue
                for fname in os.listdir(d):
                    if not fname.lower().endswith('.xml'):
                        continue
                    fpath = os.path.join(d, fname)
                    try:
                        tree = ET.parse(fpath)
                        root = tree.getroot()
                        for root_elem in root.findall('root'):
                            roots.append({
                                'val': (root_elem.get('val', '') or '').strip(),
                                'vect': (root_elem.get('vect', '') or '').strip(),
                            })
                    except Exception:
                        continue
        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¬Ø°ÙˆØ±: {e}")
        return roots

    def _root_plausibility(self, stem: str) -> int:
        """Ù‚ÙŠØ§Ø³ Ù…Ø¯Ù‰ ØªÙˆØ§ÙÙ‚ Ø§Ù„Ø¬Ø°Ø¹ Ù…Ø¹ Ø¬Ø°ÙˆØ± Ù…Ø­Ù…Ù„Ø© (Ø¨Ø­Ø±ÙˆÙ Ù…Ø±ØªØ¨Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„ÙƒÙ„Ù…Ø©)."""
        if not self.roots:
            return 0
        s = stem
        best = 0
        for r in self.roots:  # Ù†ÙØ­Øµ ÙƒÙ„ Ø§Ù„Ø¬Ø°ÙˆØ± Ù„Ø¶Ù…Ø§Ù† Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰
            val = r.get('val') or ''
            if not val:
                continue
            letters = [ch for ch in val if ch.strip()]
            # Ø¨Ø¹Ø¶ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¬Ø°ÙˆØ± Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø³Ø§ÙØ§Øª Ø¨ÙŠÙ† Ø§Ù„Ø­Ø±ÙˆÙ "Øµ Ø¯ Ù‚"
            if len(letters) >= 3:
                # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø­Ø±ÙˆÙ Ø¨ØªØ±ØªÙŠØ¨Ù‡Ø§ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¬Ø°Ø¹
                idx = 0
                ok = True
                for ch in letters:
                    pos = s.find(ch, idx)
                    if pos == -1:
                        ok = False
                        break
                    idx = pos + 1
                if ok:
                    best = max(best, len(letters))
        return best * 100

    def _class_compat_score(self, prefix_list: List[str], suffix_list: List[str], stem: Optional[str] = None, pattern_types: Optional[List[str]] = None) -> int:
        """ØªÙ‚Ø¯ÙŠØ± ØªÙˆØ§ÙÙ‚ ÙØ¦Ø§Øª Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ ÙˆØ§Ù„Ù„ÙˆØ§Ø­Ù‚ Ù…Ø¹ ØªØ®Ù…ÙŠÙ† Ø§Ø³Ù…/ÙØ¹Ù„ (ØªÙ‚Ø±ÙŠØ¨ Ø¯Ù‚ÙŠÙ‚).
        - ÙŠØ±Ø§Ø¹ÙŠ Ø¹Ø§Ø¦Ù„Ø§Øª Ø§Ù„ÙØ¦Ø§Øª: C*, N*, V*
        - ÙŠÙØ¶Ù‘Ù„ ØªØ±ØªÙŠØ¨ (Ùˆ/Ù) Ø«Ù… (Ø¨/Ùƒ/Ù„/Ø³) Ø«Ù… (Ø§Ù„)
        - ÙŠÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø¯Ù„Ø§Ø¦Ù„ Ø§Ù„Ø§Ø³Ù…ÙŠØ© (Ø§Ù„ØŒ ÙˆÙ†/ÙŠÙ†/Ø§ØªØŒ N*) ÙˆØ§Ù„ÙØ¹Ù„ÙŠØ© (V*ØŒ Ø³ÙˆØ§Ø¨Ù‚ ØµØ±ÙÙŠØ© ÙØ¹Ù„ÙŠØ©)
        """
        def fam(c: str) -> str:
            return (c or '')[:1]

        pref_classes = [self._pref_class.get(p) or '' for p in prefix_list]
        suf_classes = [self._suf_class.get(s) or '' for s in suffix_list]

        score = 0

        # 1) ØªØ±ØªÙŠØ¨ Ø§Ù„Ø³ÙˆØ§Ø¨Ù‚ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ø£Ù„ÙˆÙ
        if prefix_list:
            if prefix_list[0] in ('Ùˆ', 'Ù'):
                score += 10
            if any(p in ('Ø¨', 'Ùƒ', 'Ù„', 'Ø³') for p in prefix_list[1:]):
                score += 8
            if prefix_list[-1] == 'Ø§Ù„':
                score += 12

        # 2) Ø¯Ù„Ø§Ø¦Ù„ Ø§Ù„Ø§Ø³Ù…ÙŠØ©/Ø§Ù„ÙØ¹Ù„ÙŠØ© Ù…Ù† Ø§Ù„ÙØ¦Ø§Øª ÙˆØ§Ù„Ø³Ø·Ø­ÙŠØ§Øª
        noun_signals = 0
        verb_signals = 0

        if 'Ø§Ù„' in prefix_list:
            noun_signals += 2
        if any(s in ('ÙˆÙ†', 'ÙŠÙ†', 'Ø§Øª') for s in suffix_list):
            noun_signals += 2
        if any(c.startswith('N') for c in suf_classes) or any(c.startswith('N') for c in pref_classes):
            noun_signals += 1

        if any(c.startswith('V') for c in suf_classes) or any(c.startswith('V') for c in pref_classes):
            verb_signals += 1
        if stem and len(stem) >= 3 and stem[0] in ('ÙŠ', 'Øª', 'Ø£', 'Ù†') and 'Ø§Ù„' not in prefix_list:
            verb_signals += 1
        # Ø¯Ù„Ø§Ø¦Ù„ Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ù† Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
        if pattern_types:
            if any(t and 'verb' in t.lower() for t in pattern_types):
                verb_signals += 2
            if any(t and 'noun' in t.lower() for t in pattern_types):
                noun_signals += 2

        # ØªØ±Ø¬ÙŠØ­ Ø§Ù„ÙØ¦Ø© Ø§Ù„ØºØ§Ù„Ø¨Ø© ÙˆÙ…Ø¹Ø§Ù‚Ø¨Ø© Ø§Ù„ØªØ¶Ø§Ø¯
        if noun_signals and verb_signals:
            score -= 8
        elif noun_signals > verb_signals:
            score += 10
        elif verb_signals > noun_signals:
            score += 10

        # 3) ØªØ¬Ø§Ù†Ø³ Ø¹Ø§Ø¦Ù„Ø§Øª Ø§Ù„Ù„ÙˆØ§Ø­Ù‚
        fam_suf = [fam(c) for c in suf_classes if c]
        if fam_suf and len(set(fam_suf)) == 1:
            score += 6

        # 4) Ù‚ÙŠÙˆØ¯ Ø¹Ø¯Ù… Ø§Ù„ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©
        if 'Ø§Ù„' in prefix_list and any(c.startswith('V') for c in suf_classes):
            score -= 15  # "Ø§Ù„" Ù…Ø¹ Ù„Ø§Ø­Ù‚Ø© ÙØ¹Ù„ÙŠØ© ØºÙŠØ± Ø´Ø§Ø¦Ø¹

        # 5) Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ø¬Ù‡ÙˆÙ„Ø©
        if any(not c for c in pref_classes):
            score -= 2
        if any(not c for c in suf_classes):
            score -= 2

        return score

    # ØªØ·Ø¨ÙŠØ¹ Ù…Ø¨Ø³Ø· Ù„Ù„Ø£ÙØ¹Ø§Ù„ Ø§Ù„Ù…Ø¹ØªÙ„Ø©/Ø§Ù„Ø¥Ø¹Ù„Ø§Ù„: Ù†Ø­Ø§ÙˆÙ„ Ø£Ø´ÙƒØ§Ù„Ù‹Ø§ Ø¨Ø¯ÙŠÙ„Ø© Ù„Ù„Ø¬Ø°Ø¹ Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¬Ø°Ø±
    def _normalize_weak_stems(self, stem: str) -> List[str]:
        forms = {stem}
        s = stem
        # 0) ØªØ·Ø¨ÙŠØ¹ Ø¹Ø§Ù…: Ø§Ù„Ù‡Ù…Ø²Ø§ØªØŒ Ø§Ù„Ø£Ù„Ù Ø§Ù„Ù…Ù‚ØµÙˆØ±Ø©ØŒ Ø§Ù„ØªØ§Ø¡ Ø§Ù„Ù…Ø±Ø¨ÙˆØ·Ø©
        hamza_map = {
            'Ø£': 'Ø§', 'Ø¥': 'Ø§', 'Ø¢': 'Ø§',
            'Ø¤': 'Ùˆ', 'Ø¦': 'ÙŠ', 'Ø¡': ''
        }
        if any(ch in s for ch in hamza_map):
            t = ''.join(hamza_map.get(ch, ch) for ch in s)
            forms.add(t)
        if 'Ù‰' in s:
            forms.add(s.replace('Ù‰', 'ÙŠ'))
        if 'Ø©' in s:
            forms.add(s.replace('Ø©', 'Ù‡'))
            forms.add(s.replace('Ø©', ''))

        # 1) Ù‚Ù„Ø¨ Ø§Ù„Ø£Ù„Ù Ø¥Ù„Ù‰ ÙˆØ§Ùˆ/ÙŠØ§Ø¡ Ø­Ø³Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚ (ØªØ¨Ø³ÙŠØ·)
        if 'Ø§' in s:
            forms.add(s.replace('Ø§', 'Ùˆ'))
            forms.add(s.replace('Ø§', 'ÙŠ'))

        # 2) Ø­Ø°Ù Ø­Ø±Ù Ø§Ù„Ø¹Ù„Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        if s and s[-1] in 'Ø§ÙˆÙŠÙ‰':
            forms.add(s[:-1])

        # 3) Ø­Ø°Ù Ø­Ø±Ù Ø¹Ù„Ø© Ø£ÙˆØ³Ø· Ù…ÙØ±Ø¯
        for i, ch in enumerate(s):
            if ch in 'Ø§ÙˆÙŠÙ‰' and 0 < i < len(s) - 1:
                forms.add(s[:i] + s[i+1:])

        # 4) Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØªØ¶Ø¹ÙŠÙ (Ø§Ù„ØªÙƒØ±Ø§Ø±): Ø¥Ø²Ø§Ù„Ø© Ø£Ø­Ø¯ Ø§Ù„Ø­Ø±ÙÙŠÙ† Ø§Ù„Ù…ØªØ¬Ø§ÙˆØ±ÙŠÙ†
        for i in range(1, len(s)):
            if s[i] == s[i-1]:
                forms.add(s[:i] + s[i+1:])

        return list(forms)

    # ------------------------------
    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±
    # ------------------------------
    _AR_DIAC = ''.join([
        '\u064B', '\u064C', '\u064D', '\u064E', '\u064F', '\u0650', '\u0651', '\u0652',
        '\u0670', '\u0653', '\u0654', '\u0655'
    ])

    def _strip_diacritics(self, text: str) -> str:
        if not text:
            return text
        return re.sub(f"[{self._AR_DIAC}]", '', text)

    def _extract_root_via_patterns(self, stem: str) -> List[Dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø± Ù…Ù† Ø§Ù„Ø¬Ø°Ø¹ Ø¨Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø· (Ù/Ø¹/Ù„/Ù„) Ø¨Ø¹Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ù…Ù† Ø§Ù„Ø£Ù†Ù…Ø§Ø·.
        ÙŠØ¹ÙŠØ¯ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†: [{'root': 'Øµ Ø¯ Ù‚', 'pattern_id': '...', 'pattern': 'ÙØ§Ø¹Ù„'}]
        """
        candidates: List[Dict] = []
        if not self.patterns:
            return candidates

        # Ù†Ø­Ø§ÙˆÙ„ Ø£ÙŠØ¶Ø§Ù‹ Ø¹Ù„Ù‰ Ø¬Ø°Ø¹ Ù…Ù†Ø²ÙˆØ¹ "Ø§Ù„" Ø§Ù„ØªØ¹Ø±ÙŠÙ Ø¥Ù† ÙˆÙØ¬Ø¯
        stems_to_try = [stem]
        if stem.startswith('Ø§Ù„') and len(stem) > 2:
            stems_to_try.append(stem[2:])

        for st in stems_to_try:
            for pat in self.patterns[:50000]:  # Ø­Ø¯ Ø£Ù…Ø§Ù†
                p = self._strip_diacritics(pat.get('diac') or '')
                if not p:
                    continue
                # Ø¨Ù†Ø§Ø¡ ØªØ¹Ø¨ÙŠØ± Ù†Ù…Ø·ÙŠ Ø¨Ø§Ø³ØªØ¨Ø¯Ø§Ù„ placeholders (Ù/Ø¹/Ù„/Ù„)
                # Ù†Ø¯Ø¹Ù… Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ (Ù/Ø¹/Ù„) ÙˆØ§Ù„Ø±Ø¨Ø§Ø¹ÙŠ (Ù/Ø¹/Ù„/Ù„)
                placeholders = []
                regex_parts = []
                for ch in p:
                    if ch == 'Ù':
                        placeholders.append('R')
                        regex_parts.append('([\u0621-\u064A])')
                    elif ch == 'Ø¹':
                        placeholders.append('R')
                        regex_parts.append('([\u0621-\u064A])')
                    elif ch == 'Ù„':
                        placeholders.append('R')
                        regex_parts.append('([\u0621-\u064A])')
                    else:
                        # Ø­Ø±Ù Ø«Ø§Ø¨Øª Ù…Ù† Ø§Ù„Ù†Ù…Ø·
                        regex_parts.append(re.escape(ch))
                # ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ·Ø§Ø¨Ù‚ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„ÙƒØ§Ù…Ù„
                rx = '^' + ''.join(regex_parts) + '$'
                m = re.match(rx, st)
                if not m:
                    continue
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©
                groups = list(m.groups())
                if len(groups) < 3:
                    continue
                # Ø«Ù„Ø§Ø«ÙŠ: Ø£ÙˆÙ„ 3 Ù…Ø¬Ù…ÙˆØ¹Ø§ØªØ› Ø±Ø¨Ø§Ø¹ÙŠ: 4 Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
                if len(groups) >= 4:
                    root_letters = [groups[0], groups[1], groups[2], groups[3]]
                else:
                    root_letters = [groups[0], groups[1], groups[2]]
                # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¬Ø°Ø± ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¬Ø°ÙˆØ± (Ù…Ø¹ Ø£Ùˆ Ø¨Ø¯ÙˆÙ† Ù…Ø³Ø§ÙØ§Øª)
                root_no_space = ''.join(root_letters)
                root_spaced = ' '.join(root_letters)
                exists = False
                for r in self.roots[:5000]:
                    val = (r.get('val') or '').replace(' ', '')
                    if val == root_no_space:
                        exists = True
                        break
                candidates.append({
                    'root': root_spaced if exists else root_spaced,
                    'pattern_id': pat.get('id'),
                    'pattern': p,
                    'type': pat.get('type'),
                    'exists': exists,
                    'cas': pat.get('cas'),
                    'ncg': pat.get('ncg'),
                    'trans': pat.get('trans')
                })

        # Ù†ÙØ¶Ù„ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ø§Ù„Ø°ÙŠÙ† ÙˆÙØ¬Ø¯ÙˆØ§ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¬Ø°ÙˆØ±
        candidates.sort(key=lambda x: (1 if x['exists'] else 0), reverse=True)
        # Ù†Ø¹ÙŠØ¯ Ø£ÙØ¶Ù„ 3
        return candidates[:3]
    
    def _load_toolwords(self) -> List[Dict]:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        toolwords = []
        try:
            tree = ET.parse(os.path.join(self.db_path, 'underived', 'toolwords.xml'))
            root = tree.getroot()
            
            for toolword in root.findall('toolword'):
                toolwords.append({
                    'unvoweled': toolword.get('unvoweledform', ''),
                    'voweled': toolword.get('voweledform', ''),
                    'type': toolword.get('type', ''),
                    'prefix_class': toolword.get('prefixeclass', ''),
                    'suffix_class': toolword.get('suffixeclass', '')
                })
        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©: {e}")
        
        return toolwords
    
    def analyze_word(self, word: str) -> List[Dict]:
        """ØªØ­Ù„ÙŠÙ„ ÙƒÙ„Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù†Ù‡Ø¬ Ø§Ù„Ø®Ù„ÙŠÙ„ Ø§Ù„Ø£ØµÙ„ÙŠ"""
        word = word.strip()
        if not word:
            return []
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ Ù…Ù† Ø§Ù„Ù…ÙØ¯Ø®Ù„ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø´ÙƒÙˆÙ„Ø©
        normalized = self._strip_diacritics(word)

        results = []
        
        # 1. Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£ÙˆÙ„Ø§Ù‹
        toolword_results = self._analyze_toolwords(normalized)
        if toolword_results:
            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙƒÙ„Ù…Ø© Ø£Ø¯Ø§Ø© (Ù…Ø«Ù„ "ÙÙŠ") Ù†ÙƒØªÙÙŠ Ø¨Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£Ø¯Ø§Ø© Ù„ØªØ¬Ù†Ù‘Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø± ØºÙŠØ± Ø§Ù„Ù…ÙÙŠØ¯
            return toolword_results
        
        # 2. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
        morphological_results = self._analyze_morphology(normalized)
        results.extend(morphological_results)
        
        # 3. Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ØŒ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø¬Ø°ÙˆØ± Ù…Ø¨Ø§Ø´Ø±Ø©
        if not results:
            root_results = self._analyze_roots(normalized)
            results.extend(root_results)
        
        return results
    
    def _analyze_toolwords(self, word: str) -> List[Dict]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©"""
        results = []
        for toolword in self.toolwords:
            if word == toolword['unvoweled']:
                results.append({
                    'type': 'toolword',
                    'word': word,
                    'voweled': toolword['voweled'],
                    'toolword_type': toolword['type'],
                    'prefix_class': toolword['prefix_class'],
                    'suffix_class': toolword['suffix_class'],
                    'analysis': f"ÙƒÙ„Ù…Ø© Ù…Ø³Ø§Ø¹Ø¯Ø©: {word} ({toolword['type']})"
                })
        return results
    
    def _analyze_morphology(self, word: str) -> List[Dict]:
        """Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµØ±ÙÙŠ Ù„Ù„ÙƒÙ„Ù…Ø©"""
        results = []

        # 1) ØªÙˆÙ„ÙŠØ¯ ÙƒÙ„ Ø§Ù„ØªØ±ÙƒÙŠØ¨Ø§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø© Ù„Ù„Ø³ÙˆØ§Ø¨Ù‚ ÙˆÙÙ‚ ØªØ±ØªÙŠØ¨ Ø¹Ø±Ø¨ÙŠ Ù…Ù†Ø·Ù‚ÙŠ: [Ùˆ/Ù] Ø«Ù… [Ø¨/Ùƒ/Ù„/Ø³] Ø«Ù… [Ø§Ù„]
        stage1 = ['Ùˆ', 'Ù', '']
        stage2 = ['Ø¨', 'Ùƒ', 'Ù„', 'Ø³', '']
        stage3 = ['Ø§Ù„', '']

        prefix_candidates: List[Tuple[List[str], str]] = []  # (prefix_list, remaining_after)
        for p1 in stage1:
            for p2 in stage2:
                for p3 in stage3:
                    seq = [x for x in [p1, p2, p3] if x]
                    pref_str = ''.join(seq)
                    if pref_str and word.startswith(pref_str):
                        prefix_candidates.append((seq, word[len(pref_str):]))
        # Ø¥Ø¶Ø§ÙØ© Ø®ÙŠØ§Ø± Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø³ÙˆØ§Ø¨Ù‚
        prefix_candidates.append(([], word))

        # 2) ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ±ÙƒÙŠØ¨Ø§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø© Ù„Ù„ÙˆØ§Ø­Ù‚: [Ø¬Ù…Ø¹ (ÙˆÙ†/ÙŠÙ†/Ø§Øª)] Ø«Ù… [Ø¶Ù…ÙŠØ± (Ù‡/Ù‡Ø§/Ù‡Ù…/Ù‡Ù†/Ùƒ/ÙƒÙ…/ÙƒÙ†/ÙŠ/Ù†Ø§)]ØŒ Ù…Ø¹ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ù„Ø§ Ø£ÙŠ Ù„Ø§Ø­Ù‚Ø©
        plurals = ['ÙˆÙ†', 'ÙŠÙ†', 'Ø§Øª', '']
        pronouns = ['ÙƒÙ…Ø§', 'Ù‡Ù…Ø§', 'ÙƒÙ…', 'ÙƒÙ†', 'Ù‡Ù…', 'Ù‡Ù†', 'Ù‡Ø§', 'Ù‡', 'Ù†Ø§', 'ÙŠ', 'Ùƒ', '']  # Ø§Ù„Ø£Ø·ÙˆÙ„ Ø£ÙˆÙ„Ø§Ù‹

        best = None
        best_score = -1

        # ØªØ¬Ù…ÙŠØ¹ ÙƒÙ„ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† ÙˆØªÙ‚ÙŠÙŠÙ…Ù‡Ù… Ø«Ù… Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„ ÙˆÙÙ‚ Ø­Ø¯ Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø¬ÙˆØ¯Ø©
        candidates_ranked: List[Tuple[int, Tuple[List[str], str, List[str], Dict]]] = []
        for pref_list, after_pref in prefix_candidates:
            if not after_pref:
                continue
            for pl in plurals:
                for pr in pronouns:
                    suf_seq = [x for x in [pl, pr] if x]
                    suf_str = ''.join(suf_seq)
                    if suf_str:
                        if after_pref.endswith(suf_str):
                            stem = after_pref[:-len(suf_str)]
                        else:
                            continue
                    else:
                        stem = after_pref
                    # Ø´Ø±ÙˆØ· ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„Ø¬Ø°Ø¹
                    if not stem or len(stem) < 2:
                        continue
                    # Ø¯Ø±Ø¬Ø© Ù…Ù„Ø§Ø¡Ù…Ø©:
                    arabic = all('\u0600' <= ch <= '\u06FF' for ch in stem)
                    if not arabic or len(stem) < 2:
                        continue
                    # Ø¥Ø²Ø§Ù„Ø© ØªÙØ¶ÙŠÙ„ Ø§Ù„Ø·ÙˆÙ„: Ù„Ø§ Ù†ÙƒØ§ÙØ¦ Ø§Ù„Ø¬Ø°Ø¹ Ø§Ù„Ø£Ø·ÙˆÙ„ ÙƒÙŠ Ù„Ø§ Ù†ÙØ¨Ù‚ÙŠ "Ø§Ù„" Ø¯Ø§Ø®Ù„Ù‡
                    score = 0
                    if 3 <= len(stem) <= 6:
                        score += 15
                    # ØªÙˆØ§ÙÙ‚ Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
                    score += self._root_plausibility(stem)
                    # ØªÙˆØ§ÙÙ‚ Ø§Ù„Ø¬Ø°ÙˆØ± Ø¨Ø¹Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ù„Ù„Ø£ÙØ¹Ø§Ù„ Ø§Ù„Ù…Ø¹ØªÙ„Ø© (Ù†Ø£Ø®Ø° Ø£ÙØ¶Ù„ Ø¨Ø¯ÙŠÙ„ ÙÙ‚Ø·)
                    alt_scores = [self._root_plausibility(alt) for alt in self._normalize_weak_stems(stem)]
                    if alt_scores:
                        score += int(max(alt_scores) * 0.5)
                    # Ù†Ù‚Ø§Ø· ÙˆØ¬ÙˆØ¯ ØªØ·Ø§Ø¨Ù‚ Ù†Ù…Ø·ÙŠ ÙØ¹Ù„ÙŠ
                    pattern_hits = self._extract_root_via_patterns(stem)
                    pattern_types = [c.get('type') for c in pattern_hits] if pattern_hits else []
                    if pattern_hits:
                        if any(c.get('exists') for c in pattern_hits):
                            score += 140
                        else:
                            score += 70
                    # ØªÙˆØ§ÙÙ‚ Ø§Ù„ÙØ¦Ø§Øª (Ù†Ù…Ø±Ø± Ø§Ù„Ø¬Ø°Ø¹ ÙˆØ£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø·)
                    score += self._class_compat_score(pref_list, suf_seq, stem, pattern_types)
                    # Ù…ÙƒÙˆÙ†Ø§Øª Ø¹Ø±Ø¨ÙŠØ© Ø´Ø§Ø¦Ø¹Ø©
                    if pref_list:
                        score += 10
                    if pl:
                        score += 60
                    if pr:
                        score += 30
                    if 'Ø§Ù„' in pref_list:
                        score += 30
                    # Ø¹Ù‚ÙˆØ¨Ø© Ø¥Ø¨Ù‚Ø§Ø¡ Ø³ÙˆØ§Ø¨Ù‚/Ù„ÙˆØ§Ø­Ù‚ Ø¸Ø§Ù‡Ø±Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¬Ø°Ø¹ Ø¨Ø¯ÙˆÙ† ÙØµÙ„
                    if stem.startswith('Ø§Ù„') and 'Ø§Ù„' not in pref_list:
                        score -= 120
                    if stem.startswith('Ø§Ù„') and any(x in ('Ø¨','Ùƒ','Ù„','Ø³') for x in pref_list) and 'Ø§Ù„' not in pref_list:
                        score -= 50
                    if stem.endswith(('ÙˆÙ†','ÙŠÙ†','Ø§Øª')) and not any(x in ('ÙˆÙ†','ÙŠÙ†','Ø§Øª') for x in suf_seq):
                        score -= 50
                    if stem and any(stem.startswith(p + 'Ø§Ù„') for p in ('Ø¨','Ùƒ','Ù„','Ø³')) and 'Ø§Ù„' not in pref_list:
                        score -= 30
                    # ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ Ø­Ø±Ù Ø¹Ø·Ù Ø«Ù… "Ø§Ù„" Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¬Ø°Ø¹ØŒ Ø§Ù„Ø£ÙØ¶Ù„ ÙØµÙ„Ù‡Ø§ ÙƒØ³ÙˆØ§Ø¨Ù‚
                    if (stem.startswith('ÙˆØ§Ù„') or stem.startswith('ÙØ§Ù„')) and 'Ø§Ù„' not in pref_list:
                        score -= 60
                    # Ù…ÙƒØ§ÙØ£Ø© Ù„ØªÙ‚Ø³ÙŠÙ… ØºÙ†ÙŠ: ÙˆØ¬ÙˆØ¯ Ùˆ/Ù + (Ø¨/Ùƒ/Ù„/Ø³) + Ø§Ù„ + Ø¬Ù…Ø¹
                    if any(x in ('Ùˆ','Ù') for x in pref_list) and any(x in ('Ø¨','Ùƒ','Ù„','Ø³') for x in pref_list) and 'Ø§Ù„' in pref_list and any(x in ('ÙˆÙ†','ÙŠÙ†','Ø§Øª') for x in suf_seq):
                        score += 40
                    # Ø®Ø²Ù‘Ù† Ø§Ù„Ù…Ø±Ø´Ø­ Ù„Ù„ØªØµÙ†ÙŠÙ Ù„Ø§Ø­Ù‚Ù‹Ø§
                    candidates_ranked.append((score, (pref_list, stem, suf_seq, {'pattern_hits': pattern_hits})))

        # Ø§Ø®ØªØ± Ø£ÙØ¶Ù„ Ù…Ø±Ø´Ø­ ÙŠØªØ¬Ø§ÙˆØ² Ø­Ø¯Ù‹Ø§ Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø¬ÙˆØ¯Ø©ØŒ ÙˆØ¥Ù„Ø§ Ø§Ø®ØªØ± Ø§Ù„Ø£Ø¹Ù„Ù‰
        if candidates_ranked:
            candidates_ranked.sort(key=lambda x: x[0], reverse=True)
            MIN_SCORE = 170  # Ø±ÙØ¹ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ø¨Ø¹Ø¯ ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø£ÙˆØ²Ø§Ù†
            chosen_score, chosen = candidates_ranked[0]
            # Ø§Ø¨Ø­Ø« Ø¹Ù† Ø£ÙˆÙ„ Ù…Ø±Ø´Ø­ ÙŠØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯
            for sc, cand in candidates_ranked:
                if sc >= MIN_SCORE:
                    chosen_score, chosen = sc, cand
                    break
            pref_list, stem, suf_seq, aux = chosen
            pattern_roots = aux.get('pattern_hits') or []
            stem_analysis = self._analyze_stem(stem)
            if pattern_roots:
                stem_analysis.setdefault('via_patterns', pattern_roots)
            results.append({
                'type': 'morphological',
                'prefixes': pref_list,
                'suffixes': suf_seq,
                'stem': stem,
                'stem_analysis': stem_analysis,
                'analysis': f"Ø³ÙˆØ§Ø¨Ù‚: {'+'.join(pref_list) if pref_list else 'Ù„Ø§ ÙŠÙˆØ¬Ø¯'} + Ø¬Ø°Ø¹: {stem} + Ù„ÙˆØ§Ø­Ù‚: {'+'.join(suf_seq) if suf_seq else 'Ù„Ø§ ÙŠÙˆØ¬Ø¯'}"
            })

        return results
    
    def _analyze_stem(self, stem: str) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø°Ø¹ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¬Ø°Ø± ÙˆØ§Ù„Ù†Ù…Ø·"""
        analysis = {
            'possible_roots': [],
            'possible_patterns': [],
            'length': len(stem)
        }
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø¬Ø°ÙˆØ±
        for root in self.roots:
            if root['val'] and stem == root['val']:
                analysis['possible_roots'].append({
                    'root': root['val'],
                    'vect': root['vect'],
                    'type': 'exact_match'
                })
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†Ù…Ø§Ø· Ù…Ø·Ø§Ø¨Ù‚Ø©
        for pattern in self.patterns:
            if pattern['diac'] and stem == pattern['diac']:
                analysis['possible_patterns'].append({
                    'id': pattern['id'],
                    'pattern': pattern['diac'],
                    'type': pattern['type'],
                    'aug': pattern['aug'],
                    'cas': pattern['cas'],
                    'ncg': pattern['ncg'],
                    'trans': pattern['trans']
                })
        
        return analysis
    
    def _analyze_roots(self, word: str) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± ÙÙŠ Ø§Ù„Ø¬Ø°ÙˆØ±"""
        results = []
        for root in self.roots:
            if root['val'] and word == root['val']:
                results.append({
                    'type': 'root_direct',
                    'word': word,
                    'root': root['val'],
                    'vect': root['vect'],
                    'analysis': f"Ø¬Ø°Ø± Ù…Ø¨Ø§Ø´Ø±: {word}"
                })
        return results
